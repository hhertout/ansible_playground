tls:
  cert_manager:
    issuer: letsencrypt-issuer

alloy:
  name: alloy
  kind: Deployment
  specs:
    replicas: 1
    container_name: alloy
    image: grafana/alloy:v1.9.2
    ports:
      - port: 4317
        protocol: TCP
        exposed_port: 4317
        name: grpc
      - port: 4318
        protocol: TCP
        exposed_port: 4318
        name: http
      - port: 9999
        protocol: TCP
        exposed_port: 9999
        name: pyroscope
  volume:
    config:
      name: alloy-config
      configMap: alloy-config
      path: /etc/config.alloy
      subPath: config.alloy
  ingress:
    http:
      hosts:
        - host: otel-http.neryolab.com
          path: /
          service_name: alloy-service
          tls_secret_name: alloy-http-tls
          port: 4318
    grpc:
      hosts:
        - host: otel-grpc.neryolab.com
          path: /
          service_name: alloy-service
          tls_secret_name: alloy-grpc-tls
          port: 4317
        - host: otel-pyroscope.neryolab.com
          path: /
          service_name: alloy-service
          tls_secret_name: alloy-pyroscope-tls
          port: 9999
  hpa:
    min: 1
    max: 2
    cpu: 120
    memory: 120

alloy_config: |
    logging {
        level = "info"
        format = "logfmt"
        write_to = [loki.write.default.receiver]
    }

    otelcol.receiver.otlp "default" {
        grpc {
            endpoint = "0.0.0.0:4317"
        }
        http {
            endpoint = "0.0.0.0:4318"
        }

        output {
            metrics = [otelcol.processor.batch.default.input]
            logs    = [otelcol.processor.batch.default.input]
            traces  = [otelcol.processor.batch.default.input]
        }
    }

    otelcol.processor.batch "default" {
        output {
            metrics = [otelcol.exporter.prometheus.mimir.input]
            logs    = [otelcol.exporter.loki.default.input]
            traces  = [otelcol.exporter.otlp.tempo.input]
        }
    }

    otelcol.exporter.otlp "tempo" {
        client {
            endpoint = "tempo-service:3200"
            tls {
                insecure = true
            }
        }
    }

    otelcol.exporter.loki "default" {
        forward_to = [loki.write.default.receiver]
    }

    loki.write "default" {
        endpoint {
            url = "http://loki-service:3100/loki/api/v1/push"

            tls_config {
                insecure_skip_verify = true
            }
        }
    }

    otelcol.exporter.prometheus "mimir" {
        forward_to = [prometheus.remote_write.mimir.receiver]
    }

    prometheus.remote_write "mimir" {
        endpoint {
            url = "http://mimir-service:9009/api/v1/push"
        }
    }

    pyroscope.receive_http "default" {
        http {
            listen_address = "0.0.0.0"
            listen_port = 9999
        }

        forward_to = [pyroscope.relabel.filter_profiles.receiver]
        }

        pyroscope.relabel "filter_profiles" {
            forward_to = [pyroscope.write.backend.receiver]

            rule {
                action = "labeldrop"
                regex = "service_name"
            }
        }

        // Forwards profiles to Pyroscope
        pyroscope.write "backend" {
        endpoint {
            url = "http://pyroscope-service:4040"
        }
    }

    prometheus.exporter.self "default" {
    }

    prometheus.relabel "self_monitor_relabel" {
        forward_to = [prometheus.remote_write.mimir.receiver]


        rule {
            action        = "replace"
            target_label  = "job"
            replacement   = "alloy"
        }

        rule {
            action        = "replace"
            target_label  = "service_name"
            replacement   = "alloy"
        }
    }

    prometheus.scrape "metamonitoring" {
        targets    = prometheus.exporter.self.default.targets
        forward_to = [prometheus.relabel.self_monitor_relabel.receiver]
    }

grafana:
  name: grafana
  kind: Deployment
  specs:
    replicas: 1
    container_name: grafana
    image: grafana/grafana:latest
    container_port: 3000
    exposed_port: 3000
  volume:
    name: grafana-pvc
    storage: 1Gi
  ingress:
    host: grafana.neryolab.com
  hpa:
    min: 1
    max: 2
    cpu: 120
    memory: 120

grafana_provisioning:
  volume:
    config:
      name: grafana-provisioning-conf
      configMap: grafana-provisioning-config
      path: /etc/grafana/provisioning/datasources/datasources.yaml
      subPath: datasources.yaml

loki:
  name: loki
  kind: StatefulSet
  specs:
    replicas: 1
    container_name: loki
    image: grafana/loki:3.4
    container_port: 3100
    exposed_port: 3100
    args:
      - value: "-config.file=/etc/loki.yaml"
  volume:
    config:
      name: loki-config
      configMap: loki-config
      path: /etc/loki.yaml
      subPath: loki.yaml
  hpa:
    min: 1
    max: 1
    cpu: 120
    memory: 120

loki_config: |
    auth_enabled: false

    server:
      http_listen_port: 3100
      log_level: info

    common:
      instance_addr: 127.0.0.1
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        kvstore:
          store: inmemory

    limits_config:
      allow_structured_metadata: true
      volume_enabled: true

    ruler:
      storage:
        type: local
        local:
          directory: /etc/loki/rules
      ring:
        kvstore:
          store: memberlist
      rule_path: /tmp/loki/scratch
      alertmanager_url: https://alertmanager.xx
      external_url: https://alertmanager.xx

    ingester:
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        final_sleep: 0s
      chunk_idle_period: 1h
      max_chunk_age: 1h
      chunk_target_size: 1048576
      chunk_retain_period: 30s

    schema_config:
      configs:
        - from: 2020-10-24
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h

    storage_config:
      tsdb_shipper:
        active_index_directory: /tmp/loki/index
        cache_location: /tmp/loki/index_cache
      filesystem:
        directory: /tmp/loki/chunks

    pattern_ingester:
      enabled: true

mimir:
  name: mimir
  kind: StatefulSet
  specs:
    replicas: 1
    container_name: mimir
    image: grafana/mimir:2.15.1
    args:
      - value: -target=all
      - value: -config.expand-env=true
      - value: -config.file=/etc/mimir.yaml
    container_port: 9009
    exposed_port: 9009
  volume:
    config:
      name: mimir-config
      configMap: mimir-config
      path: /etc/mimir.yaml
      subPath: mimir.yaml
  hpa:
    min: 1
    max: 1
    cpu: 120
    memory: 120

mimir_config: |
    multitenancy_enabled: false

    blocks_storage:
      backend: filesystem
      bucket_store:
        sync_dir: /tmp/mimir/tsdb-sync
      filesystem:
        dir: /tmp/mimir/data/tsdb
      tsdb:
        dir: /tmp/mimir/tsdb

    compactor:
      data_dir: /tmp/mimir/compactor
      sharding_ring:
        kvstore:
          store: memberlist

    distributor:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist

    ingester:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist
        replication_factor: 1

    ruler_storage:
      backend: filesystem
      filesystem:
        dir: /tmp/mimir/rules

    server:
      http_listen_port: 9009
      log_level: info

    store_gateway:
      sharding_ring:
        replication_factor: 1

    limits:
    # A maximum of 100000 exemplars in memory at any one time.
    # This setting enables exemplar processing and storage.
      max_global_exemplars_per_user: 100000
      ingestion_rate: 30000


tempo:
  name: tempo
  kind: StatefulSet
  specs:
    replicas: 1
    container_name: tempo
    image: grafana/tempo:2.7.1
    ports:
      - port: 3100
        protocol: TCP
        exposed_port: 3100
        name: http
      - port: 3200
        protocol: TCP
        exposed_port: 3200
        name: grpc
    args:
      - value: "-config.file=/etc/tempo.yaml"
  volume:
    config:
      name: tempo-config
      configMap: tempo-config
      path: /etc/tempo.yaml
      subPath: tempo.yaml
    override:
      name: override-config
      configMap: tempo-config
      path: /conf/overrides.yaml
      subPath: overrides.yaml
  hpa:
    min: 1
    max: 1
    cpu: 120
    memory: 120

tempo_config:
  memBallastSizeMbs: 1024
  multitenancyEnabled: false
  reportingEnabled: false
  metricsGenerator:
    enabled: true
    remoteWriteUrl: "http://mimir-service:9002/api/v1/write"
  ingester:
    flush_check_period: 10s
    trace_idle_period: 10s
    max_block_duration: 30m
    complete_block_timeout: 1h
  querier:
    max_concurrent_queries: 20
  queryFrontend:
   search:
     concurrent_jobs: 2000
  retention: 24h
  overrides:
    defaults:
     metrics_generator:
       processors:
         - service-graphs
         - span-metrics
    per_tenant_override_config: /conf/overrides.yaml
  per_tenant_overrides:
    'tenant-id':
     metrics_generator:
       processors:
         - service-graphs
         - span-metrics
  server:
    http_listen_port: 3100
    grpc_listen_port: 3200
    log_level: info
  storage:
    trace:
      backend: local
      local:
        path: /var/tempo/traces
      wal:
        path: /var/tempo/wal
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"

tempo_configmap: |
    memberlist:
      cluster_label: "{{ .Release.Name }}.{{ .Release.Namespace }}"
    multitenancy_enabled: {{ .Values.tempo_config.multitenancyEnabled }}
    usage_report:
      reporting_enabled: {{ .Values.tempo_config.reportingEnabled }}
    compactor:
      compaction:
        block_retention: {{ .Values.tempo_config.retention }}
    distributor:
      receivers:
        {{- toYaml .Values.tempo_config.receivers | nindent 8 }}
    ingester:
      {{- toYaml .Values.tempo_config.ingester | nindent 6 }}
    server:
      {{- toYaml .Values.tempo_config.server | nindent 6 }}
    storage:
      {{- toYaml .Values.tempo_config.storage | nindent 6 }}
    querier:
      {{- toYaml .Values.tempo_config.querier | nindent 6 }}
    query_frontend:
      {{- toYaml .Values.tempo_config.queryFrontend | nindent 6 }}
    overrides:
      {{- toYaml .Values.tempo_config.overrides | nindent 6 }}
      {{- if .Values.tempo_config.metricsGenerator.enabled }}
    metrics_generator:
          storage:
            path: "/tmp/tempo"
            remote_write:
              - url: {{ .Values.tempo_config.metricsGenerator.remoteWriteUrl }}
      {{- end }}


pyroscope:
  name: pyroscope
  kind: StatefulSet
  specs:
    container_name: pyroscope
    image: grafana/pyroscope:1.8.0
    container_port: 4040
    exposed_port: 4040
  hpa:
    min: 1
    max: 1
    cpu: 120
    memory: 120